{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1: Capstone Project Task 0\n",
    "### Introduction\n",
    "This task has 2 goals\n",
    "1. Obtain data from the course website\n",
    "1. Familiarize yourself with background  \n",
    "\n",
    "    2.1 What do the data look like?\n",
    "\n",
    "    2.2 Where do the data come from?\n",
    "\n",
    "    2.3 Can you think of any other data sources that might help you in this project?\n",
    "\n",
    "    2.4 What are the common steps in natural language processing?\n",
    "\n",
    "    2.5 What are some common issues in the analysis of text data?\n",
    "\n",
    "    2.6 What is the relationship between NLP and the concepts you have learned in the Specialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Environmet Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘igraph’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    decompose, spectrum\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:base’:\n",
      "\n",
      "    union\n",
      "\n",
      "\n",
      "\n",
      "Attaching package: ‘influenceR’\n",
      "\n",
      "\n",
      "The following objects are masked from ‘package:igraph’:\n",
      "\n",
      "    betweenness, constraint\n",
      "\n",
      "\n",
      "Loading required package: NLP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(data.tree)\n",
    "library(DiagrammeR)\n",
    "library(igraph)\n",
    "library(influenceR)\n",
    "library(plyr)\n",
    "library(SnowballC)\n",
    "library(stringi)\n",
    "library(tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the google profanity word list from this websit <a href=\"https://github.com/RobertJGabriel/Google-profanity-words\">https://github.com/RobertJGabriel/Google-profanity-words</a> using `git clone`, and the file is `list.txt` respectively. Move it to the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity_words <- read.table(\"~/Soft/Rtest/JHU_capstone_project_data/list.txt\", header = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtaining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set work directory accordingly\n",
    "setwd(\"~/Soft/Rtest/datasciencecoursera/10_Capstone_Project/\")\n",
    "\n",
    "# download the dataset\n",
    "url = \"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\"\n",
    "#download.file(url, \n",
    "#              destfile = \"data/Coursera-SwiftKey.zip\"\n",
    "#             )\n",
    "\n",
    "# unzip the file\n",
    "# unzip(\"./data/Coursera-SwiftKey.zip\", exdir = \"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the files into R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsch\n",
    "blogs_DE <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.blogs.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "news_DE <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.news.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "twitter_DE <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.twitter.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "\n",
    "# Finnish\n",
    "blogs_FI <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.blogs.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "news_FI <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.news.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "twitter_FI <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.twitter.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "\n",
    "# Russian\n",
    "blogs_RU <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.blogs.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "news_RU <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.news.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "twitter_RU <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.twitter.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "\n",
    "# English\n",
    "blogs_US <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.blogs.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "news_US <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.news.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n",
    "twitter_US <- readLines(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\"\n",
    "                     , encoding = \"UTF-8\"\n",
    "                     , skipNul = TRUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What do the data look like?\n",
    "We try to show 3 characteristics of the data\n",
    "- dendrogram tree of the data files\n",
    "- number of lines by type and by country\n",
    "- size of data sets by type and by country\n",
    "- wordcounts by type and by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/latex": [],
      "text/markdown": [],
      "text/plain": [
       "character(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir(\"data/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dendrogram tree of the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list <- c(\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.blogs.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.news.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.twitter.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.blogs.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.news.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.twitter.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.blogs.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.news.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.twitter.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.blogs.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.news.txt\",\n",
    "    \"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                   levelName\n",
       "1  ~                                        \n",
       "2   °--Soft                                 \n",
       "3       °--Rtest                            \n",
       "4           °--JHU_capstone_project_data    \n",
       "5               °--final                    \n",
       "6                   ¦--de_DE                \n",
       "7                   ¦   ¦--de_DE.blogs.txt  \n",
       "8                   ¦   ¦--de_DE.news.txt   \n",
       "9                   ¦   °--de_DE.twitter.txt\n",
       "10                  ¦--fi_FI                \n",
       "11                  ¦   ¦--fi_FI.blogs.txt  \n",
       "12                  ¦   ¦--fi_FI.news.txt   \n",
       "13                  ¦   °--fi_FI.twitter.txt\n",
       "14                  ¦--ru_RU                \n",
       "15                  ¦   ¦--ru_RU.blogs.txt  \n",
       "16                  ¦   ¦--ru_RU.news.txt   \n",
       "17                  ¦   °--ru_RU.twitter.txt\n",
       "18                  °--en_US                \n",
       "19                      ¦--en_US.blogs.txt  \n",
       "20                      ¦--en_US.news.txt   \n",
       "21                      °--en_US.twitter.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_tree <- lapply(strsplit(file_list, \"/\"), function(z) as.data.frame(t(z)))\n",
    "file_tree <- rbind.fill(file_tree)\n",
    "file_tree$pathString <- apply(file_tree, 1, function(z) paste(trimws(na.omit(z)), collapse = \"/\"))\n",
    "my_tree <- data.tree::as.Node(file_tree)\n",
    "my_tree # unix tree-command like style\n",
    "#plot(my_tree) # more elegant style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(my_tree, file = \"data/my_tree.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of lines by type and by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df <- data.frame(\n",
    "    country = \"DE\"\n",
    "    , blogs = c(length(blogs_DE))\n",
    "    , news = c(length(news_DE))\n",
    "    , twitter = c(length(twitter_DE))\n",
    ")\n",
    "length_df <- rbind(length_df, c(\"FI\", length(blogs_FI), length(news_FI), length(twitter_FI)))\n",
    "length_df <- rbind(length_df, c(\"RU\", length(blogs_RU), length(news_RU), length(twitter_RU)))\n",
    "length_df <- rbind(length_df, c(\"US\", length(blogs_US), length(news_US), length(twitter_US)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the data sets' length (lines) of each type (blogs, news, twitter) by country is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  blogs    news twitter\n",
      "1      DE 371440  244743  947774\n",
      "2      FI 439785  485758  285214\n",
      "3      RU 337100  196360  881414\n",
      "4      US 899288 1010242 2360148\n"
     ]
    }
   ],
   "source": [
    "print(length_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(length_df, file = \"data/length_df.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of data sets by type and by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsch\n",
    "blogs_size_DE <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.blogs.txt\")$size/(1024.0^2)\n",
    "news_size_DE <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.news.txt\")$size/(1024.0^2)\n",
    "twitter_size_DE <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/de_DE/de_DE.twitter.txt\")$size/(1024.0^2)\n",
    "\n",
    "# Finnish\n",
    "blogs_size_FI <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.blogs.txt\")$size/(1024.0^2)\n",
    "news_size_FI <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.news.txt\")$size/(1024.0^2)\n",
    "twitter_size_FI <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/fi_FI/fi_FI.twitter.txt\")$size/(1024.0^2)\n",
    "\n",
    "# Russian\n",
    "blogs_size_RU <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.blogs.txt\")$size/(1024.0^2)\n",
    "news_size_RU <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.news.txt\")$size/(1024.0^2)\n",
    "twitter_size_RU <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/ru_RU/ru_RU.twitter.txt\")$size/(1024.0^2)\n",
    "\n",
    "# English\n",
    "blogs_size_US <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.blogs.txt\")$size/(1024.0^2)\n",
    "news_size_US <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.news.txt\")$size/(1024.0^2)\n",
    "twitter_size_US <- file.info(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\")$size/(1024.0^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_df <- data.frame(\n",
    "    country = \"DE\"\n",
    "    , blogs = round(blogs_size_DE, 2)\n",
    "    , news = round(news_size_DE, 2)\n",
    "    , twitter = round(twitter_size_DE,2)\n",
    ")\n",
    "size_df <- rbind(size_df, c(\"FI\", round(blogs_size_FI, 2), round(news_size_FI,2), round(twitter_size_FI,2)))\n",
    "size_df <- rbind(size_df, c(\"RU\", round(blogs_size_RU, 2), round(news_size_RU,2), round(twitter_size_RU,2)))\n",
    "size_df <- rbind(size_df, c(\"US\", round(blogs_size_US, 2), round(news_size_US,2), round(twitter_size_US,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the data sets' size (MBs) of each type (blogs, news, twitter) by country is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  blogs   news twitter\n",
      "1      DE   81.5  91.16   72.08\n",
      "2      FI 103.48  89.87   24.16\n",
      "3      RU 111.44 113.48  100.31\n",
      "4      US 200.42 196.28  159.36\n"
     ]
    }
   ],
   "source": [
    "print(size_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(size_df, file = \"data/size_df.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordcounts by type and by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_words_DE <-  sum(sapply(gregexpr(\"\\\\S+\", blogs_DE), length))\n",
    "news_words_DE <-  sum(sapply(gregexpr(\"\\\\S+\", news_DE), length))\n",
    "twitter_words_DE <-  sum(sapply(gregexpr(\"\\\\S+\", twitter_DE), length))\n",
    "\n",
    "blogs_words_FI <-  sum(sapply(gregexpr(\"\\\\S+\", blogs_FI), length))\n",
    "news_words_FI <-  sum(sapply(gregexpr(\"\\\\S+\", news_FI), length))\n",
    "twitter_words_FI <-  sum(sapply(gregexpr(\"\\\\S+\", twitter_FI), length))\n",
    "\n",
    "blogs_words_RU <-  sum(sapply(gregexpr(\"\\\\S+\", blogs_RU), length))\n",
    "news_words_RU <-  sum(sapply(gregexpr(\"\\\\S+\", news_RU), length))\n",
    "twitter_words_RU <-  sum(sapply(gregexpr(\"\\\\S+\", twitter_RU), length))\n",
    "\n",
    "blogs_words_US <-  sum(sapply(gregexpr(\"\\\\S+\", blogs_US), length))\n",
    "news_words_US <-  sum(sapply(gregexpr(\"\\\\S+\", news_US), length))\n",
    "twitter_words_US <-  sum(sapply(gregexpr(\"\\\\S+\", twitter_US), length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df <- data.frame(\n",
    "    country = \"DE\"\n",
    "    , blogs = round(blogs_words_DE, 2)\n",
    "    , news = round(news_words_DE, 2)\n",
    "    , twitter = round(twitter_words_DE,2)\n",
    ")\n",
    "words_df <- rbind(words_df, c(\"FI\", blogs_words_FI, news_words_FI, twitter_words_FI))\n",
    "words_df <- rbind(words_df, c(\"RU\", blogs_words_RU, news_words_RU, twitter_words_RU))\n",
    "words_df <- rbind(words_df, c(\"US\", blogs_words_US, news_words_US, twitter_words_US))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that the data sets' word counts of each type (blogs, news, twitter) by country is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country    blogs     news  twitter\n",
      "1      DE 12653019 13219287 11803491\n",
      "2      FI 12731005 10445964  3152758\n",
      "3      RU  9405378  9115829  9223841\n",
      "4      US 37334131 34372530 30373583\n"
     ]
    }
   ],
   "source": [
    "print(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveRDS(words_df, file = \"data/words_df.RDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Where do the data come from?\n",
    "\n",
    "The data come from **SwiftKey**, and the url is \n",
    "<a href=\"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\">https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip</a>\n",
    "\n",
    "### 2.3 Can you think of any other data sources that might help you in this project?\n",
    "\n",
    "Yes. For example, the WIKIPEDIA is a good text corpus for NLP\n",
    "\n",
    "### 2.4 What are the common steps in natural language processing?\n",
    "According to <a href=\"https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e\">https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e</a>, generally speaking NLP takes the common steps\n",
    "- Sentence Segmentation\n",
    "- Word Tokenization\n",
    "- Predicting Parts of Speech for Each Token\n",
    "- Text Lemmatization\n",
    "- Stop Words Identification\n",
    "- Dependency Parsing\n",
    "- Finding Noun Phrases\n",
    "- Named Entity Recognition (NER)\n",
    "- Conference Resolution\n",
    "\n",
    "### 2.5 What are some common issues in the analysis of text data?\n",
    "The common issues in the analysis of text data are:\n",
    "- Grammar mistakes: e.g. Father and Fahter\n",
    "- Capital and Lower words: e.g. World and world\n",
    "- Words of similar meanings: e.g. UK, Britan, GB ...\n",
    "\n",
    "### 2.6 What is the relationship between NLP and the concepts you have learned in the Specialization?\n",
    "\n",
    "NLP use algorithms similar to those used in ML, for example, such as statistical inference\n",
    "\n",
    "The difference is mainly that they deal with varing objects, e.g. housing price prediction model mainly focuses on price which is number, and NLP mainly focuses on letter / words / sentence which is character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone Project Task 1\n",
    "\n",
    "**Tasks to accomplish**\n",
    "\n",
    "1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.\n",
    "1. Profanity filtering - removing profanity and other words you do not want to predict.\n",
    "\n",
    "**Tips, tricks, and hints**\n",
    "\n",
    "1. Loading the data in. This dataset is fairly large. We emphasize that you don't necessarily need to load the entire dataset in to build your algorithms (see point 2 below). At least initially, you might want to use a smaller subset of the data. Reading in chunks or lines using R's readLines or scan functions can be useful. You can also loop over each line of text by embedding readLines within a for/while loop, but this may be slower than reading in large chunks at a time. Reading pieces of the file at a time will require the use of a file connection in R. For example, the following code could be used to read the first few lines of the English Twitter dataset:con <- file(\"en_US.twitter.txt\", \"r\") readLines(con, 1) ## Read the first line of text readLines(con, 1) ## Read the next line of text readLines(con, 5) ## Read in the next 5 lines of text close(con) ## It's important to close the connection when you are done. See the connections help page for more information.\n",
    "2. Sampling. To reiterate, to build models you don't need to load in and use all of the data. Often relatively few randomly selected rows or chunks need to be included to get an accurate approximation to results that would be obtained using all the data. Remember your inference class and how a representative sample can be used to infer facts about a population. You might want to create a separate sub-sample dataset by reading in a random subset of the original data and writing it out to a separate file. That way, you can store the sample and not have to recreate it every time. You can use the rbinom function to \"flip a biased coin\" to determine whether you sample a line of text or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_blogs_US <- blogs_US[sample(1:length(blogs_US), 10000)]\n",
    "sample_news_US <- news_US[sample(1:length(news_US), 10000)]\n",
    "sample_twitter_US <- twitter_US[sample(1:length(twitter_US), 10000)]\n",
    "\n",
    "sample_sum_US <- c(sample_blogs_US, sample_news_US, sample_twitter_US)\n",
    "\n",
    "sum_US <- c(blogs_US, news_US, twitter_US)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the source to be character vectors\n",
    "sample_sum_US <- Corpus(VectorSource(sample_sum_US))\n",
    "sum_US <- Corpus(VectorSource(sum_US))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "sample_sum_US <- tm_map(\n",
    "    sample_sum_US\n",
    "    , content_transformer(tolower)\n",
    "#    , lazy = TRUE\n",
    ")\n",
    "sum_US <- tm_map(\n",
    "                sum_US\n",
    "                , content_transformer(tolower)\n",
    "                #    , lazy = TRUE\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_sum_US\n",
    "\n",
    "# remove punction\n",
    "sample_sum_US <- tm_map(sample_sum_US, content_transformer(removePunctuation))\n",
    "\n",
    "# remove Numbers\n",
    "sample_sum_US <- tm_map(sample_sum_US, content_transformer(removeNumbers))\n",
    "\n",
    "# remove URLs\n",
    "removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\n",
    "sample_sum_US <- tm_map(sample_sum_US, content_transformer(removeURL))\n",
    "\n",
    "# remove whitespace\n",
    "sample_sum_US <- tm_map(sample_sum_US, content_transformer(stripWhitespace))\n",
    "\n",
    "# remove stopwords with english\n",
    "sample_sum_US <- tm_map(sample_sum_US, removeWords, stopwords(\"english\"))\n",
    "\n",
    "# remove profanity words\n",
    "sample_sum_US <- tm_map(sample_sum_US, removeWords, profanity_words[,1])\n",
    "\n",
    "#stemDocument\n",
    "sample_sum_US <- tm_map(sample_sum_US, stemDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_US\n",
    "\n",
    "# remove punction\n",
    "sum_US <- tm_map(sum_US, content_transformer(removePunctuation))\n",
    "\n",
    "# remove Numbers\n",
    "sum_US <- tm_map(sum_US, content_transformer(removeNumbers))\n",
    "\n",
    "# remove URLs\n",
    "# removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\n",
    "sum_US <- tm_map(sum_US, content_transformer(removeURL))\n",
    "\n",
    "# remove whitespace\n",
    "sum_US <- tm_map(sum_US, content_transformer(stripWhitespace))\n",
    "\n",
    "# remove stopwords with english\n",
    "sum_US <- tm_map(sum_US, removeWords, stopwords(\"english\"))\n",
    "\n",
    "# remove profanity words\n",
    "sum_US <- tm_map(sum_US, removeWords, profanity_words[,1])\n",
    "\n",
    "#stemDocument\n",
    "sum_US <- tm_map(sum_US, stemDocument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "writeLines(sample_sum_US, \"~/Soft/Rtest/JHU_capstone_project_data/final/sample_sum_US.txt\")\n",
    "writeLines(sum_US, \"~/Soft/Rtest/JHU_capstone_project_data/final/sum_US.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "What is the length of the longest line seen in any of the three en_US data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.blogs.txt\")\n",
    "lenn <- nchar(readLines(con = con))\n",
    "max_len_blogs_US <- max(lenn)\n",
    "close(con)\n",
    "max_len_blogs_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\", \"r\")\n",
    "lenn <- nchar(readLines(con = con))\n",
    "max_len_twitter_US <- max(lenn)\n",
    "close(con)\n",
    "max_len_twitter_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.news.txt\", \"r\")\n",
    "lenn <- nchar(readLines(con = con))\n",
    "max_len_news_US <- max(lenn)\n",
    "close(con)\n",
    "max_len_news_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "\n",
    "In the en_US twitter data set, if you divide the number of lines where the word “love” (all lowercase) occurs by the number of lines the word “hate” (all lowercase) occurs, about what do you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\", \"r\")\n",
    "lovec <- grepl(\".love.\"#\".[Ll][Oo][Vv][Ee].\"\n",
    "              , readLines(con)\n",
    "               , ignore.case = FALSE\n",
    "              )\n",
    "lovec <- sum(lovec)\n",
    "close(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\", \"r\")\n",
    "hatec <- grepl(\".hate.\"#\".[Hh][Aa][Tt][Ee].\"\n",
    "               , readLines(con)\n",
    "               , ignore.case = FALSE\n",
    "              )\n",
    "hatec <- sum(hatec)\n",
    "hatec\n",
    "close(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lovec/hatec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: \n",
    "The one tweet in the en_US twitter data set that matches the word “biostats” says what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\", \"r\")\n",
    "readfile <- readLines(con)\n",
    "readfile[grepl(\".biostats.\", readfile, ignore.case = FALSE) == TRUE]\n",
    "close(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "How many tweets have the exact characters \"A computer once beat me at chess, but it was no match for me at kickboxing\". (I.e. the line matches those characters exactly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con <- file(\"~/Soft/Rtest/JHU_capstone_project_data/final/en_US/en_US.twitter.txt\", \"r\")\n",
    "readfile <- readLines(con)\n",
    "readfile[\n",
    "    grepl(\"A computer once beat me at chess, but it was no match for me at kickboxing\"\n",
    "         , readfile\n",
    "         , ignore.case = FALSE) == TRUE\n",
    "]\n",
    "close(con)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
